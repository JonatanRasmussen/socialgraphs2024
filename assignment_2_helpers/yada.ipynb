{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Stats of the Country Music Network \n",
    "\n",
    "Part 2 of the assignment covers the following two exercises from week 4 of the course:\n",
    "This second part requires you to have built the network of Country Musicians as described in the exercises for Week 4. You should complete the following exercise from **Part 2**.\n",
    "- A. Simple network statistics and analysis\n",
    "- B. Build a simple visualization of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2A. Simple network statistics and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with the necessary imports and after that the construction of the API url for the wiki page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/w/api.php?action=query&titles=List_of_country_music_performers&prop=revisions&rvprop=content&format=json\n"
     ]
    }
   ],
   "source": [
    "#Remove all variable declarations from Part1\n",
    "for name in dir():\n",
    "    if not name.startswith('_'):\n",
    "        del globals()[name]\n",
    "\n",
    "import urllib3, re, json, requests, random\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from urllib.parse import quote\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "\n",
    "baseurl = \"https://en.wikipedia.org/w/api.php?\"\n",
    "action = \"action=query\"\n",
    "title = \"titles=List_of_country_music_performers\"\n",
    "content = \"prop=revisions&rvprop=content\"\n",
    "dataform = \"format=json\"\n",
    "query = \"%s%s&%s&%s&%s\" % (baseurl,action,title,content,dataform)\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we call this API to get this page's information. After its transformation into json format we convert it to a string form in order for us to be able to extract the performers links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/w/api.php?action=query&titles=List_of_country_music_performers&prop=revisions&rvprop=content&format=json\n"
     ]
    }
   ],
   "source": [
    "import urllib3, re, json, requests, random\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from urllib.parse import quote\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "\n",
    "baseurl = \"https://en.wikipedia.org/w/api.php?\"\n",
    "action = \"action=query\"\n",
    "title = \"titles=List_of_country_music_performers\"\n",
    "content = \"prop=revisions&rvprop=content\"\n",
    "dataform = \"format=json\"\n",
    "query = \"%s%s&%s&%s&%s\" % (baseurl,action,title,content,dataform)\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we call this API to get this page's information. After its transformation into json format we convert it to a string form in order for us to be able to extract the performers links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "http = urllib3.PoolManager()\n",
    "wikiresponse = http.request('GET', query)\n",
    "wikisource = wikiresponse.data.decode('utf-8')\n",
    "\n",
    "wikijson = json.loads(wikisource)\n",
    "text = wikijson[\"query\"][\"pages\"]\n",
    "wiki_text = json.dumps(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to use regex on this string in order to extract only the needed information, which in this case are the performers links. We can see that all the words that are a link are inside double brackets (for example [[The Abrams Brothers]]). Thus, it will be easy to extract all the links using \"findall\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = re.findall(r\"\\[\\[(.*?)\\]\\]\", wiki_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to do some cleaning to the names of the performers. First and foremost, we are replacing all white spaces with underscores, otherwise when we transform these names into URLs we might not be able to find all of the pages. Furthermore, there are many images and other file links in the performer list page, which we can just remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = [name.replace(\" \", \"_\") for name in results if not (name.startswith(\"File:\") or name.startswith(\"Image:\"))]\n",
    "cleaned_data = cleaned_data[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many performers have special characters, but since there are so many it is a kind of the trouble to find all of them. Just by looking at the names, the most common one is double backslash \"\\\\\\\\\", which we are going to remove. Also, some names have a vertical bar \"|\" and after that bar the simple name is represented, but after testing it out it has no influence in the construction of the performer names URLs, so there is no need to remove it. Last but not least, we noticed that some performers' URL do not actually have the name shown in the list above, but it is impossible to go one by one and check every performer's link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_singers = [name.replace(\"\\\\\", \"\") for name in cleaned_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now we can start getting the information of every performer in the list. We do that with a loop where the API of each performer is build and using it we get the page's information, which is transformed into a string. The result of this loop is a list of all the performer names and their information in json format converted to text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "performer_text = {}\n",
    "for singer in list_of_singers:\n",
    "    baseurl = \"https://en.wikipedia.org/w/api.php?\"\n",
    "    action = \"action=query\"\n",
    "    title = f\"titles={quote(singer)}\"\n",
    "    content = \"prop=revisions&rvprop=content\"\n",
    "    dataform = \"format=json\"\n",
    "    query = f\"{baseurl}{action}&{title}&{content}&{dataform}\"\n",
    "    response = requests.get(query, timeout=10)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        #imported code from previously\n",
    "        wikiresponse = http.request('GET', query)\n",
    "        wikisource = wikiresponse.data.decode('utf-8')\n",
    "        wikijson = json.loads(wikisource)\n",
    "        text = wikijson[\"query\"][\"pages\"]\n",
    "        wiki_text = json.dumps(text)\n",
    "        performer_text[singer] = wiki_text\n",
    "    else:\n",
    "        print(f\"Error fetching data for {singer}: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use regex again and extract all the links in each performer wiki page using findall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "performer_contents = {}\n",
    "for performer, content in performer_text.items():\n",
    "    performer_contents[performer] = re.findall(r\"\\[\\[(.*?)\\]\\]\", content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We go again for the same transformation for the links, in order for them to be in the same form as the performer names. Thus, if a link is another performer's name we can find it and match it easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for performer, links in performer_contents.items():\n",
    "        new_values = [link.replace(\" \", \"_\").replace(\"\\\\\", \"\") for link in links]\n",
    "\n",
    "        performer_contents[performer] = new_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we do the matching of links with the performer names and store all the matched pairs in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = []\n",
    "for performer, links in performer_contents.items():\n",
    "    for link in links:\n",
    "        if link in performer_contents:\n",
    "            matches.append((performer, link))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And remove the duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = list(set(matches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to start building the network, which has the performer names as nodes and the matched pairs as directed edges. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "G.add_nodes_from(performer_contents.keys())\n",
    "G.add_edges_from(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of nodes: 2103\n",
      "Total number of edges: 17517\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of nodes:\", G.number_of_nodes())\n",
    "print(\"Total number of edges:\", G.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started with the statistics now. First, let's plot the distributions for in and out degree. As we can see both follow a power law distribution where we have some extreme points (nodes) with a big in or out degree number, while the majority of nodes have a very small degree number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "in_degrees = [d for n, d in G.in_degree()]\n",
    "out_degrees = [d for n, d in G.out_degree()]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(in_degrees, bins=100, color='blue')\n",
    "plt.title('In-Degree Distribution')\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Number of nodes')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(out_degrees, bins=100, color='darkorange')\n",
    "plt.title('Out-Degree Distribution')\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Number of nodes')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare the out-degree of this network with the out-degree of a completely random directed network we would see major diffences. The degree distribution of a random network follows a poisson distribution, where the degree of most nodes is close to the average and there are not many nodes with extremely low or high degree. We can double check it by ploting the out-degree distribution of a random network with the same number of nodes and edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "D = nx.gnm_random_graph(2013, 17509, directed=True)\n",
    "\n",
    "out_degrees_D = [d for n, d in D.out_degree()]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(out_degrees_D, bins=50, color='darkorange')\n",
    "plt.title('Out-Degree Distribution')\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Number of nodes')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, if we compared the in-degree distribution with a scale free network with the same number of nodes, we would see that the distributions are almost identical, as the scale free network also follows a power law distribution. In the plot below, we can see a log-log scale, where if we exclude the few nodes on the bottom right corner with high in-degree that can be considered as hubs, the rest of the nodes form an approximate straight line, suggesting that they indeed follow a power law distribution like the scale free network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
   ],
   "source": [
    "degree_counts = np.bincount(in_degrees)\n",
    "degree_values = np.arange(len(degree_counts))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.loglog(degree_values, degree_counts, 'bo', markersize=4, label=\"Data points\")\n",
    "\n",
    "plt.title(\"Log-Log Plot\")\n",
    "plt.xlabel(\"Degree (k)\")\n",
    "plt.ylabel(\"P(k)\")\n",
    "plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can see the performers with the most in and out degree. In the top 5 of in-degree there are some very famous performers such as Elvis Presley, Johnny Cash and Dolly Parton. This is expected, as performers with a high in-degree can be considered very popular and their names are referenced frequently on other performers' pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Performers by in-degree:\n",
      "Willie_Nelson: 202\n",
      "Johnny_Cash: 185\n",
      "Elvis_Presley: 173\n",
      "Dolly_Parton: 161\n",
      "Merle_Haggard: 159\n",
      "\n",
      "Top 5 Performers by out-degree:\n",
      "Hillary_Lindsey: 97\n",
      "Pam_Tillis: 82\n",
      "Randy_Travis: 75\n",
      "Vince_Gill: 71\n",
      "Patty_Loveless: 67\n"
     ]
    }
   ],
   "source": [
    "in_degree = dict(G.in_degree())\n",
    "out_degree = dict(G.out_degree())\n",
    "\n",
    "top_5_in = sorted(in_degree.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "top_5_out = sorted(out_degree.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "print(\"Top 5 Performers by in-degree:\")\n",
    "for performer, in_degree in top_5_in:\n",
    "    print(f\"{performer}: {in_degree}\")\n",
    "\n",
    "print(\"\\nTop 5 Performers by out-degree:\")\n",
    "for performer, out_degree in top_5_out:\n",
    "    print(f\"{performer}: {out_degree}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in order to find the length of each performer's page, we are going to use the list from before, where we store all the information from the API. Once again, we are using findall to count the words of each page. It will not be completely accurate, since as we said previously there are some performers that use another name from the one in the list of \"country music performers\" and thus no information could be found through their API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_size = []\n",
    "\n",
    "for performer, content in performer_text.items():\n",
    "    words = re.findall(r'\\w+', content)\n",
    "    content_size.append((performer,len(words)))\n",
    "\n",
    "#content_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 10 performers with the longest wiki entries are displayed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Performers by length of wiki page:\n",
      "Taylor_Swift: 56923\n",
      "Miley_Cyrus: 51750\n",
      "Justin_Bieber: 51170\n",
      "Carrie_Underwood: 45906\n",
      "Justin_Timberlake: 45205\n",
      "Demi_Lovato: 44441\n",
      "Alabama_(band)|Alabama: 44304\n",
      "Bob_Dylan: 41806\n",
      "Ed_Sheeran: 36819\n",
      "Elvis_Presley: 34565\n"
     ]
    }
   ],
   "source": [
    "top_10_content = sorted(content_size, key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "print(\"Top 10 Performers by length of wiki page:\")\n",
    "for performer, content in top_10_content:\n",
    "    print(f\"{performer}: {content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see the in and out degree of these top 10 performers, but with a quick look there is not a very strong correlation between length and the degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Taylor_Swift', 56923): in-degree:[77] and out-degree:[27]\n",
      "('Miley_Cyrus', 51750): in-degree:[17] and out-degree:[11]\n",
      "('Justin_Bieber', 51170): in-degree:[15] and out-degree:[11]\n",
      "('Carrie_Underwood', 45906): in-degree:[68] and out-degree:[61]\n",
      "('Justin_Timberlake', 45205): in-degree:[13] and out-degree:[14]\n",
      "('Demi_Lovato', 44441): in-degree:[9] and out-degree:[7]\n",
      "('Alabama_(band)|Alabama', 44304): in-degree:[26] and out-degree:[24]\n",
      "('Bob_Dylan', 41806): in-degree:[141] and out-degree:[21]\n",
      "('Ed_Sheeran', 36819): in-degree:[14] and out-degree:[8]\n",
      "('Elvis_Presley', 34565): in-degree:[173] and out-degree:[21]\n"
     ]
    }
   ],
   "source": [
    "for performer in top_10_content:\n",
    "    in_top10 = [d for n, d in G.in_degree(performer)]\n",
    "    out_top10 = [d for n, d in G.out_degree(performer)]\n",
    "    print(f\"{performer}: in-degree:{in_top10} and out-degree:{out_top10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2B. Build a simple visualization of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving to the network visualization, we first have to convert the previous network (G) to an undirected network. Then, we have to collect the total degree of each node as it will be used as the node size later. The average node degree is ~14 so if we use a multiplier of 1.5 the average node size would be also around 20, which is pretty good visually (we tested some other multipliers but it gets messy when >2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.088445078459344\n"
     ]
    }
   ],
   "source": [
    "G_undirected = G.to_undirected()\n",
    "node_degrees = dict(G_undirected.degree())\n",
    "print(sum(node_degrees.values()) / len(node_degrees))\n",
    "list_of_sizes = [degree * 1.5 for degree in node_degrees.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The node color depends on the length of the performers' wiki page. We will use a reversed blues scale, where the dark blue means small number of word count and light blue to white means high word count. This way it is easy to distinguish all the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_sizes = dict(content_size).values()\n",
    "norm = mpl.colors.Normalize(vmin=min(node_sizes), vmax=max(node_sizes))\n",
    "cmap = plt.cm.Blues_r\n",
    "node_colors = [cmap(norm(size)) for size in node_sizes]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we visualize the network. The visualization can be seen below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a giant component in the middle, where most hubs (popular performers) are located. Another thing that can be noticed is that most of the light blue colored nodes are towards the middle and are bigger than the average. This suggests that performers with long wiki entries are more likely to be connected with other performers and this is completely normal, since if a performer has a long wiki page, he probably has many accomplishments and collaborations with other singers. Thus, there is a high probability of having other performers' names (links) inside their page. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
